{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#------------------------------\n",
    "#cpu - gpu configuration\n",
    "config = tf.ConfigProto( device_count = {'GPU': 0 , 'CPU': 56} ) #max: 1 gpu, 56 cpu\n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)\n",
    "#------------------------------\n",
    "#variables\n",
    "num_classes = 7 #angry, disgust, fear, happy, sad, surprise, neutral\n",
    "batch_size = 256\n",
    "epochs = 5\n",
    "#------------------------------\n",
    "#read kaggle facial expression recognition challenge dataset (fer2013.csv)\n",
    "#https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge\n",
    "\n",
    "file_name = '/Users/mtcd/Library/Mobile Documents/com~apple~CloudDocs/@Pessoal/@Evernote/Estudos/@programming/@testing_img_pixels/adfes_dataset.csv'\n",
    "with open(file_name) as f:\n",
    "\n",
    "    \n",
    "    content = f.readlines()\n",
    "\n",
    "lines = np.array(content)\n",
    "\n",
    "num_of_instances = lines.size\n",
    "print(\"number of instances: \",num_of_instances)\n",
    "print(\"instance length: \",len(lines[1].split(\",\")[1].split(\" \")))\n",
    "\n",
    "#------------------------------\n",
    "#initialize trainset and test set\n",
    "x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "#------------------------------\n",
    "#transfer train and test set data\n",
    "for i in range(1,num_of_instances):\n",
    "    try:\n",
    "        emotion, img, usage = lines[i].split(\",\")\n",
    "          \n",
    "        val = img.split(\" \")\n",
    "            \n",
    "        pixels = np.array(val, 'float32')\n",
    "        \n",
    "        emotion = keras.utils.to_categorical(emotion, num_classes)\n",
    "    \n",
    "        if 'Training' in usage:\n",
    "            y_train.append(emotion)\n",
    "            x_train.append(pixels)\n",
    "        elif 'PublicTest' in usage:\n",
    "            y_test.append(emotion)\n",
    "            x_test.append(pixels)\n",
    "    except:\n",
    "\t    print(\"\",end=\"\")\n",
    "\n",
    "#------------------------------\n",
    "#data transformation for train and test sets\n",
    "x_train = np.array(x_train, 'float32')\n",
    "y_train = np.array(y_train, 'float32')\n",
    "x_test = np.array(x_test, 'float32')\n",
    "y_test = np.array(y_test, 'float32')\n",
    "\n",
    "x_train /= 255 #normalize inputs between [0, 1]\n",
    "x_test /= 255\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "#------------------------------\n",
    "#construct CNN structure\n",
    "model = Sequential()\n",
    "\n",
    "#1st convolution layer\n",
    "model.add(Conv2D(64, (5, 5), activation='relu', input_shape=(48,48,1)))\n",
    "model.add(MaxPooling2D(pool_size=(5,5), strides=(2, 2)))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(AveragePooling2D(pool_size=(3,3), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "#fully connected neural networks\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "#------------------------------\n",
    "#batch process\n",
    "gen = ImageDataGenerator()\n",
    "train_generator = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "model.compile(loss='categorical_crossentropy'\n",
    "    , optimizer=keras.optimizers.Adam()\n",
    "    , metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#------------------------------\n",
    "\n",
    "fit = True\n",
    "\n",
    "if fit == True:\n",
    "\t#model.fit_generator(x_train, y_train, epochs=epochs) #train for all trainset\n",
    "\tmodel.fit_generator(train_generator, steps_per_epoch=batch_size, epochs=epochs) #train for randomly selected one\n",
    "else:\n",
    "\tmodel.load_weights('/data/facial_expression_model_weights.h5') #load weights\n",
    "\t\n",
    "#------------------------------\n",
    "\"\"\"\n",
    "#overall evaluation\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', 100*score[1])\n",
    "\"\"\"\n",
    "#------------------------------\n",
    "#function for drawing bar chart for emotion preditions\n",
    "def emotion_analysis(emotions):\n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    \n",
    "    plt.bar(y_pos, emotions, align='center', alpha=0.5)\n",
    "    plt.xticks(y_pos, objects)\n",
    "    plt.ylabel('percentage')\n",
    "    plt.title('emotion')\n",
    "    \n",
    "    plt.show()\n",
    "#------------------------------\n",
    "\n",
    "monitor_testset_results = False\n",
    "\n",
    "if monitor_testset_results == True:\n",
    "\t#make predictions for test set\n",
    "\tpredictions = model.predict(x_test)\n",
    "\n",
    "\tindex = 0\n",
    "\tfor i in predictions:\n",
    "\t\tif index < 30 and index >= 20:\n",
    "\t\t\t#print(i) #predicted scores\n",
    "\t\t\t#print(y_test[index]) #actual scores\n",
    "\t\t\t\n",
    "\t\t\ttesting_img = np.array(x_test[index], 'float32')\n",
    "\t\t\ttesting_img = testing_img.reshape([48, 48]);\n",
    "\t\t\t\n",
    "\t\t\tplt.gray()\n",
    "\t\t\tplt.imshow(testing_img)\n",
    "\t\t\tplt.show()\n",
    "\t\t\t\n",
    "\t\t\tprint(i)\n",
    "\t\t\t\n",
    "\t\t\temotion_analysis(i)\n",
    "\t\t\tprint(\"----------------------------------------------\")\n",
    "\t\tindex = index + 1\n",
    "\n",
    "#------------------------------\n",
    "#make prediction for custom image out of test set\n",
    "\n",
    "img = image.load_img(\"C:/Users/IS96273/Desktop/jackman.png\", grayscale=True, target_size=(48, 48))\n",
    "\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis = 0)\n",
    "\n",
    "x /= 255\n",
    "\n",
    "custom = model.predict(x)\n",
    "emotion_analysis(custom[0])\n",
    "\n",
    "x = np.array(x, 'float32')\n",
    "x = x.reshape([48, 48]);\n",
    "\n",
    "plt.gray()\n",
    "plt.imshow(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## FACIAL EXPRESSION MEAD DATASET ##\n",
      "\n",
      "Algorithm to analyses emotions from videos files using DeepFace Python library (https://github.com/serengil/deepface) and generate a summarise table to show how many times each emotion has captured by frame. For this examples, was used MEAD dataset (https://wywu.github.io/projects/MEAD/MEAD.html).\n",
      "\n",
      "*To use the algorithm:*\n",
      "1. Create DATASET and ANALYZED folders inside the main folder:\n",
      "\t- '/Users/abc/facial_expression_mead_dataset'\n",
      "\t- '/Users/abc/facial_expression_mead_dataset/datasets'\n",
      "\t- '/Users/abc/facial_expression_mead_dataset/analyzed'\n",
      "2. Change the path name in \"run_analyse.py\" file where videos files are in your computer\n",
      "3. Run this python file\n",
      "4. Will be create in analyzed folder\n",
      "\n",
      "\t- mead_dataset_analyzed_dict.csv (summaryzed informations from captured emotions from all videos files)\n",
      "\t- file_name.json (each video file will generate a json file from each frame emotion's captured)\n",
      "\t- file_name.txt (each video file will generate a txt file with frames, fps and duration information)\n",
      "\n",
      "5. Will be create main folder:\n",
      "\n",
      "\t- mead_dataset.db (all captured emotions are save in sqlite database)\n",
      "\n",
      "#############################################\n",
      "\n",
      "*Informations about database columns:*\n",
      "\n",
      "file_name: file name video that captured frames;\n",
      "emotion_from_mead_ds: emotion categorised by MEAD database;\n",
      "emotion_returned_from_df: emotion captured by DeepFace Python library;\n",
      "total_frames: Total frames analysed;\n",
      "angry, disgustâ€¦: Show how many frames were identified by DeepFace Python library.\n",
      "\n",
      "![Screenshot 2022-10-31 at 09 44 59](https://user-images.githubusercontent.com/77461340/198979180-c8d435b9-6530-450d-8b2f-a922a67464c6.jpg)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('README.md') as f: \n",
    "  print(f.read())\n",
    "\n",
    "  \n",
    "  #print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 'N', 'B': 'O', 'C': 'P', 'D': 'Q', 'E': 'R', 'F': 'S', 'G': 'T', 'H': 'U', 'I': 'V', 'J': 'W', 'K': 'X', 'L': 'Y', 'M': 'Z', 'N': 'A', 'O': 'B', 'P': 'C', 'Q': 'D', 'R': 'E', 'S': 'F', 'T': 'G', 'U': 'H', 'V': 'I', 'W': 'J', 'X': 'K', 'Y': 'L', 'Z': 'M', 'a': 'n', 'b': 'o', 'c': 'p', 'd': 'q', 'e': 'r', 'f': 's', 'g': 't', 'h': 'u', 'i': 'v', 'j': 'w', 'k': 'x', 'l': 'y', 'm': 'z', 'n': 'a', 'o': 'b', 'p': 'c', 'q': 'd', 'r': 'e', 's': 'f', 't': 'g', 'u': 'h', 'v': 'i', 'w': 'j', 'x': 'k', 'y': 'l', 'z': 'm'}\n"
     ]
    }
   ],
   "source": [
    "import this\n",
    "print(this.d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Zen of Python, by Tim Peters\n",
      "\n",
      "Beautiful is better than ugly.\n",
      "Explicit is better than implicit.\n",
      "Simple is better than complex.\n",
      "Complex is better than complicated.\n",
      "Flat is better than nested.\n",
      "Sparse is better than dense.\n",
      "Readability counts.\n",
      "Special cases aren't special enough to break the rules.\n",
      "Although practicality beats purity.\n",
      "Errors should never pass silently.\n",
      "Unless explicitly silenced.\n",
      "In the face of ambiguity, refuse the temptation to guess.\n",
      "There should be one-- and preferably only one --obvious way to do it.\n",
      "Although that way may not be obvious at first unless you're Dutch.\n",
      "Now is better than never.\n",
      "Although never is often better than *right* now.\n",
      "If the implementation is hard to explain, it's a bad idea.\n",
      "If the implementation is easy to explain, it may be a good idea.\n",
      "Namespaces are one honking great idea -- let's do more of those!\n"
     ]
    }
   ],
   "source": [
    "s = s = \"\"\"Gur Mra bs Clguba, ol Gvz Crgref\n",
    "\n",
    "Ornhgvshy vf orggre guna htyl.\n",
    "Rkcyvpvg vf orggre guna vzcyvpvg.\n",
    "Fvzcyr vf orggre guna pbzcyrk.\n",
    "Pbzcyrk vf orggre guna pbzcyvpngrq.\n",
    "Syng vf orggre guna arfgrq.\n",
    "Fcnefr vf orggre guna qrafr.\n",
    "Ernqnovyvgl pbhagf.\n",
    "Fcrpvny pnfrf nera'g fcrpvny rabhtu gb oernx gur ehyrf.\n",
    "Nygubhtu cenpgvpnyvgl orngf chevgl.\n",
    "Reebef fubhyq arire cnff fvyragyl.\n",
    "Hayrff rkcyvpvgyl fvyraprq.\n",
    "Va gur snpr bs nzovthvgl, ershfr gur grzcgngvba gb thrff.\n",
    "Gurer fubhyq or bar-- naq cersrenoyl bayl bar --boivbhf jnl gb qb vg.\n",
    "Nygubhtu gung jnl znl abg or boivbhf ng svefg hayrff lbh'er Qhgpu.\n",
    "Abj vf orggre guna arire.\n",
    "Nygubhtu arire vf bsgra orggre guna *evtug* abj.\n",
    "Vs gur vzcyrzragngvba vf uneq gb rkcynva, vg'f n onq vqrn.\n",
    "Vs gur vzcyrzragngvba vf rnfl gb rkcynva, vg znl or n tbbq vqrn.\n",
    "Anzrfcnprf ner bar ubaxvat terng vqrn -- yrg'f qb zber bs gubfr!\"\"\"\n",
    "\n",
    "\n",
    "d = {}\n",
    "for c in (65, 97):\n",
    "    for i in range(26):\n",
    "        d[chr(i+c)] = chr((i+13) % 26 + c)\n",
    "\n",
    "print(\"\".join([d.get(c, c) for c in s]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "function missing required argument 'database' (pos 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/mtcd/Library/Mobile Documents/com~apple~CloudDocs/@Pessoal/@Evernote/Estudos/@programming/deepface_analysis_datasets/jpty/testing.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mtcd/Library/Mobile%20Documents/com~apple~CloudDocs/%40Pessoal/%40Evernote/Estudos/%40programming/deepface_analysis_datasets/jpty/testing.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39minspect\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mtcd/Library/Mobile%20Documents/com~apple~CloudDocs/%40Pessoal/%40Evernote/Estudos/%40programming/deepface_analysis_datasets/jpty/testing.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msqlite3\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mtcd/Library/Mobile%20Documents/com~apple~CloudDocs/%40Pessoal/%40Evernote/Estudos/%40programming/deepface_analysis_datasets/jpty/testing.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(inspect\u001b[39m.\u001b[39mgetsource(sqlite3\u001b[39m.\u001b[39;49mconnect()))\n",
      "\u001b[0;31mTypeError\u001b[0m: function missing required argument 'database' (pos 1)"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import sqlite3\n",
    "print(inspect.getsource(sqlite3.connect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
